{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><font color=\"gray\">DOING COMPUTATIONAL SOCIAL SCIENCE<br>MODULE 3 <strong>PROBLEM SETS</strong></font>\n",
    "\n",
    "# <font color=\"#49699E\" size=40>Visualization & <br>Exploratory Data Analysis</font>\n",
    "\n",
    "## For UE Undergraduate Students\n",
    "\n",
    "This module notebook assignment is organized into two parts. \n",
    "\n",
    "- **[PART A](#SECA) (Accompanying Chapter 6, \"Visualization & Exploratory Data Analysis, Part A\")**\n",
    "    - [Exercises and Practice Problems](#SECAEP) (All students)\n",
    "- **[PART B](#SECB) (Accompanying Chapter 7, \"Visualization & Exploratory Data Analysis, Part B\"**)\n",
    "    - [Exercises and Practice Problems](#SECBEP) (All students)\n",
    "\n",
    "# What You Need to Know Before Getting Started\n",
    "\n",
    "- **You can consult any resources you want when completing these exercises and problems**. Just as it is in the \"real world:\" if you can't figure out how to do something, look it up. My recommendation is that you check the relevant parts of the assigned reading or search for inspiration on [https://stackoverflow.com](https://stackoverflow.com).\n",
    "- **Each problem set is worth 4 points**. All problems within the problem set are equally weighted. If there are 4 problems, then each is worth 1 point.\n",
    "- **The information you need for each problem set is provided in the blue and green cells.** General instructions / the problem set preamble are in the blue cells, and instructions for specific problems are in the green cells. **You have to execute all of the code in the problem set, but you are only responsible for entering code into the code cells that immediately follow a green cell**. You will also recognize those cells because they will be incomplete. You need to replace each `____` with the code that will make the cell execute properly.\n",
    "- **Each problem cell stores an object named according to the problem (e.g. A1B)**. These are not important for you, but we use them to help grade your work efficiently, so **do not delete them or change their names**. If you do, you will lose marks.\n",
    "- **You can ask for help**. If you run into problems, you can reach out to John (john.mclevey@uwaterloo.ca) or Pierson (pbrowne@uwaterloo.ca) for help. You can ask a friend for help if you like, regardless of whether they are enrolled in the course.\n",
    "\n",
    "Finally, remember that you do not need to \"master\" this content before moving on to other course materials, as what is introduced here is reinforced throughout the rest of the course. You will have plenty of time to practice and cement your new knowledge and skills.\n",
    "\n",
    "# How to Submit Your (Pickled) Assignment! \n",
    "\n",
    "Since we've had to rethink the way we deliver, collect, and evaluate these problem sets, we want to be very clear about what you need to do to properly submit this module notebook assignment. Please read the following explanation of our process so that you understand how this works, and what you need to do.\n",
    "\n",
    "At the very end of this notebook, there is a code cell that will compile all of your answers to every problem in the assignment and save them as a 'pickle' file (`.pkl`) in the current working directory. You can execute that cell as many times as you like. Each time you run it, it will overwrite the old pickle with your updated answers. **Once you've ensured that everything in the notebook is complete and finished to your satisfaction, it's up to you to get the pickle that you just created and upload it to the appropriate Learn dropbox for this module.** The file you are looking for will not exist until you run the cells at the end of the notebook. Once it has been created, it will follow this naming convention: \n",
    "\n",
    "> `module_[number]__student_[your_student_number].pkl`\n",
    "\n",
    "To be very clear, **you need to submit the pickle to Learn**. You do not need to upload the Jupyter Notebook as initially planned. **Just the pickle!**\n",
    "\n",
    "## Make Sure Everything is Good to Go\n",
    "\n",
    "It's generally a good idea to do a 'fresh' run of your entire notebook before you submit your assignment to make sure that everything is working as it should be. You can use the button with the 'Fast-Forward' arrows in the Jupyter toolbar above to restart the kernel (resetting everything to initial conditions) and running every code cell in the notebook, in order. You can also select 'Restart and Run All' from the Kernel dropdown menu. If the entire notebook runs without throwing any errors, you should be good to go!\n",
    "\n",
    "If you're running into issues, make sure that you haven't changed any of the 'answer' variable names we provided you with (e.g., we asked you to store your answer to the first question in a variable called `A1A`). If you change an answer's variable name or don't store your answer in that variable, the project won't finalize properly and you won't get proper credit for your work. The same goes for the `student_id` metadata variable we ask you to complete immediately below; if any of those are missing, haven't been filled in properly, or have been renamed, issues may arise during the grading process and you will not receive proper credit. So make sure you enter your student information, and don't delete or change the names of the variables that store your answers to each problem!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTANT: ADD YOUR STUDENT ID NUMBER\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "To evaluate your work, we need you to provide your student number. In the cell below, <strong>replace '12345678' with your student number</strong>. The student_id' variable needs to be an integer, so <strong>do not wrap it in quotes.</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your UWaterloo student ID number\n",
    "student_id = 12345678 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(context='paper', # scales figure elements\n",
    "              style='white', # determines the overall style\n",
    "              palette='gray', # plots will be grayscale\n",
    "              font='sans-serif', # use a sans-serif font for text\n",
    "              font_scale=.8, # increase or decrease the font size\n",
    "              color_codes=True, \n",
    "              rc={'figure.dpi':300, # display images in notebook at 300 dpi\n",
    "                  'savefig.dpi':300}) # save pngs at 300 dpi\n",
    "seed = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"SECA\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"#49699E\">PART A | CHAPTER 6</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"SECAEP\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem A1\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">  \n",
    "In this exercise, we're going to ask you to supply the names of the Pandas methods you'll need to (1) load the .csv from disk and (2) preview it. Then we'll select a subset of the columns in the dataframe.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "In the code block below, fill in the blanks to insert the functions, methods, or variable names needed to load the .csv and preview it.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A1A\n",
    "\n",
    "data_directory_path = '../input/'\n",
    "df = pd._____(data_directory_path + 'vdem_subset.csv', low_memory=False, index_col=0)\n",
    "\n",
    "A1A = df.sample(5, random_state = 7) # do not change this variable name\n",
    "\n",
    "df.sample(5, random_state = 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">  \n",
    "You may have noticed that many of the cells in the dataframe we created have 'NaN' values. It's useful for us to know just how many values in our dataset are missing or not defined. Let's do that now:\n",
    "</div>\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "In the code block below, fill in the blanks to insert the functions, methods, or variable names needed to create a Pandas series of the missing values for each column and then sort it.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A1B\n",
    "\n",
    "missing = df._____().sum()\n",
    "missing = missing.sort_values()\n",
    "\n",
    "print(missing)\n",
    "print(\"Total missing values: \" + str(sum(missing)))\n",
    "\n",
    "A1B = missing # do not change this variable name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "That's a lot of missing data! In the next problem, we'll select a subset of the measures from the VDEM dataset.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem A2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "The list below contains a number of variables, including mid-level indicators that go into the 5 high-level democracy indexes that were used in the assigned readings. In this problem, we'll subset our data in two ways - first by selecting only the mid-level indicators, and then by selecting the indicators that are on the same scale.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Use the list of column names we've provided to filter the large dataframe into a subset. Fill in the blanks to insert the functions, methods, or variable names needed.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A2A\n",
    "vd_meta_vars = ['country_name', 'year', 'e_regiongeo']\n",
    "vd_index_vars = ['v2x_freexp_altinf', 'v2x_frassoc_thick', 'v2x_suffr', 'v2xel_frefair', 'v2x_elecoff',    # electoral democracy index\n",
    "              'v2xcl_rol', 'v2x_jucon', 'v2xlg_legcon',                                                 # liberal democracy index\n",
    "              'v2x_cspart', 'v2xdd_dd', 'v2xel_locelec', 'v2xel_regelec', 'v2x_polyarchy',              # participatory democracy index\n",
    "              'v2dlreason', 'v2dlcommon', 'v2dlcountr', 'v2dlconslt', 'v2dlengage',                     # deliberative democracy index\n",
    "              'v2xeg_eqprotec', 'v2xeg_eqaccess', 'v2xeg_eqdr']                                         # egalitarian democracy index\n",
    "\n",
    "\n",
    "sdf = df[_____ + vd_index_vars]\n",
    "\n",
    "A2A = sdf # do not change this variable name\n",
    "\n",
    "sdf.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">  \n",
    "Let's dig into our subsetted data. Notice that many of the variables have a range (min to max) of 0 to 1. In the next exercises we'll be making some comparisons between variables, so let's simplify those comparisons by selecting only the meta-data columns and columns with a range of 0 to 1.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Create a new list to filter the dataframe columns, initializing it with the names of the three meta-data columns. Append that list with the names of columns from vd_index_vars, only if the data in those columns falls within the required range. Subset the dataframe using this updated list. Fill in the blanks to continue.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A2B\n",
    "\n",
    "sub_vd_indices = []\n",
    "\n",
    "for column in vd_index_vars:\n",
    "    if sdf[column].min() >= 0 and sdf[column].max() <= 1:\n",
    "        sub_vd_indices.append(_____)\n",
    "\n",
    "fsdf = sdf[vd_meta_vars + sub_vd_indices]\n",
    "\n",
    "A2B = fsdf # do not change this variable name\n",
    "\n",
    "fsdf.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem A3\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "In this problem set, we will continue to compute some descriptive statistics for our subsetted data and create some visualizations. We need a list that has the column names for just the variables, so we can re-use the 'subset_vd_indices' list from the previous problem. This code block will modify our dataframe so that it's easy to generate a single plot with all of the variables.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsdf_ecdf = fsdf[sub_vd_indices]\n",
    "fsdf_ecdf = fsdf_ecdf.melt(value_vars = sub_vd_indices, var_name = 'vd_index', value_name = 'score')\n",
    "fsdf_ecdf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Use the above dataframe to create an empirical cumulative distribution plot of the indicator variables, using hue to differentiate them. Set a title for the x and y axis to indicate what they represent by filling in the blanks.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A3A\n",
    "figure = plt.figure(figsize=(10,6))\n",
    "ax = sns.displot(fsdf_ecdf, x = 'score', hue = 'vd_index', kind = \"ecdf\")\n",
    "ax.set(xlabel=_____, ylabel=_____, title=\"ECDF for VDEM Indicator Variables\")\n",
    "figure.show()\n",
    "\n",
    "A3A = figure # do not change this variable name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "We can see that most of the variables follow a fairly smooth curve, while a few see dramatic proportion increases in a step-like way. This indicates that although we might consider these to be continuous variables, the measurement that produced them had some discrete (interval-like) qualities.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Select one variable that seems to have a relatively smooth distribution and another that has a distinctly step-like distribution. Fill their names into the blanks in the 'x' and 'y' variables in the `sns.distplot` call in the code block belpw. There are a number of options to choose from for each, so the distinction between the two is what matters. Produce a bivariate kernel density estimation rug plot for the two selected variables. Fill in the blanks to continue.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A3B\n",
    "\n",
    "figure = plt.figure()\n",
    "ax = sns.displot(fsdf, x=_____, y=_____, kind=\"kde\", rug = True, rug_kws = {\"alpha\": 0.01})\n",
    "sns.despine()\n",
    "ax.set(xlabel='frassoc_thick', ylabel='polyarchy')\n",
    "figure.show()\n",
    "\n",
    "A3B = figure # do not change this variable name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem A4\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    Now we'll create a correlation matrix (2D array) of all the variables and plot it in a heatmap to see which pairs of variables are most and least correlated. \n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsdf_corr = fsdf[sub_vd_indices].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    We'll use the following boolean mask to clean-up the heatmap. Remember that a boolean mask is an array of \"True\" and \"False\" values, the same size and shape as the data array, where a value of \"False\" indicates that the value in the data array should be ignored. In this case, the mask will remove all values on the top-left -> bottom-right diagonal and above that diagonal.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.triu(np.ones_like(fsdf_corr, dtype = bool))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Plot a heatmap of the correlation matrix. We will use this to select a few correlations to print. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure()\n",
    "ax = sns.heatmap(data = fsdf_corr, mask = mask)\n",
    "figure.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Looking at the heatmap above, select four pairs of variables that appear highly correlated and three that appear minimally correlated. Create two lists, one for the first element of each variable pair and one for the second element of each pair. These lists should be aligned, so that variable 2 of the first list will be correlated with variable 2 of the second list. Iterate over the two lists at the same time and print the Pearson correlations. What you do in the following code block won't be graded directly, but it will affect your answers in the code block after it, which will be graded.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_1_list = [_____, _____, _____, _____]\n",
    "var_2_list = [_____, _____, _____, _____]\n",
    "\n",
    "for v1, v2 in zip(var_1_list, var_2_list):\n",
    "    \n",
    "    corr = fsdf[v1].corr(fsdf[v2])\n",
    "    \n",
    "    print('Correlation of ' + v1 + ' and ' + v2 + ' : ' + str(corr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Fill in the blanks of the dictionary below with the two highest correlations and the two lowest, rounded to three decimals. If there are identical values, it's fine to just pick one of them.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A4A\n",
    "corr_dict = {'highest1': _____, 'highest2' : _____, 'lowest1' : _____, 'lowest2' : _____}\n",
    "\n",
    "A4A = corr_dict # do not change this variable name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Next, we will create a list of the 4 variable names from the most correlated and least correlated pairs. With this list, we'll create a plot of pairwise comparisons between each variable to get a sense of the relationship (or lack of relationship) they have and then indicate the direction of the highest correlation.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_vars = ['v2xdd_dd', 'v2x_suffr','v2x_frassoc_thick', 'v2x_freexp_altinf']\n",
    "\n",
    "figure = plt.figure()\n",
    "ax = sns.pairplot(fsdf[corr_vars])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Looking at the plots above, fill in the blank below indicating whether the relationship between the most correlated variables is a positive one (True or False).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A4B\n",
    "\n",
    "positive = _____\n",
    "\n",
    "A4B = positive # do not change this variable name "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"#49699E\">PART B | CHAPTER 7</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem B1\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "One useful thing that using Pandas dataframes enables us to do is group data based on one or more the columns and then work with the resulting grouped dataframe (in much the same way we would with an un-grouped dataframe). In this first section, we're going to use the VDEM data again. This time, we'll only import a subset of the data, using the 'columns_to_use' variable. At the same time, we're going to replace the numerical values in the 'e_regionpol_6c' variable with easy-to-read string representations. \n",
    "</div>\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "We're not going to grade you on this code block, and there are no blanks for you to fill in, but make sure that you run the cell and that the resulting dataframe is stored as 'df', as we're going to neeed it for the rest of the question.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_use = [\n",
    "    'country_name',\n",
    "    'country_id',\n",
    "    'year',\n",
    "    'e_area',\n",
    "    'e_regionpol_6C',\n",
    "    'v2x_polyarchy',\n",
    "    'v2x_libdem',\n",
    "    'v2x_partipdem',\n",
    "    'v2x_delibdem',\n",
    "    'v2x_egaldem'\n",
    "]\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"../input/vdem_subset.csv\",\n",
    "    usecols = columns_to_use,\n",
    "    low_memory = False\n",
    ")\n",
    "\n",
    "df['e_regionpol_6C'].replace({\n",
    "    1.0: \"East Europe and Central Asia\",\n",
    "    2.0: \"Latin America and Carribean\",\n",
    "    3.0: \"Middle East and North Africa\",\n",
    "    4.0: \"Sub-Saharan Africa\",\n",
    "    5.0: \"West Europe and North America\",\n",
    "    6.0: 'Asia and Pacific'\n",
    "}, inplace=True)\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "In addition to each of the columns we worked with before, we now have access to the 'e_area' and 'e_regionpol_6C' variables, which represent the country's total land area and their politico-geographical region. \n",
    "</div>\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "In this next code block, we're going to filter our dataframe to include only those rows where the year is 2015. Fill in the blanks to continue.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#B1A \n",
    "\n",
    "df_2015 = df._____(\"year == 2015\")\n",
    "\n",
    "B1A = df_2015 # do not change this variable name\n",
    "\n",
    "df_2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Now, we're going to use the Pandas Dataframe's `groupby` method to combine each nation into the region it belongs to. As you would have read in the accompanying chapter, the Pandas groupby method only preserves columns that you give it instructions for; everything else is dropped in the resulting dataframe. \n",
    "</div>   \n",
    "<div class=\"alert alert-block alert-info\">\n",
    "In order to figure out how to aggregate each of our columns, let's think through them together. First up, we have 'country_name' and 'country_ID'. Since we're going to be grouping our data into only 6 rows (one for each of the 6 politico-geographical regions), it doesn't make sense to keep either of these columns. The same goes for 'year', since we will have already filtered our dataset to only include rows that are from 2015. We're going to be using 'e_regionpol_6C' as the basis for our groupings, so it doesn't make sense to keep it as a data column any longer. \n",
    "</div>\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "That leaves us with 'e_area' and the 5 democracy indices. Since we're interested in knowing the total area of each region, it would make sense to *add* each country's area together. We could do something similar for the 5 democracy indices, but we'll leave them alone for now. In order to make things easier on ourselves, we're going to start by filtering out all of the columns we don't want in our final dataset, which will make aggregating what's left much easier.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "In the following code cell, we're going to filter out most of the columns in `df_2015` so that only 'e_regionpol_6C' and 'e_area' remain, and store the resulting filtered dataframe as `df_area`. Then, we're going to run a `groupby` operation on the `e_regionpol_6C` column of and sum the `e_area` column in the `df_area` dataframe. Fill in the blanks to continue. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#B1B \n",
    "df_area = df_2015[['e_regionpol_6C', 'e_area']]\n",
    "\n",
    "df_grouped_area = df_area._____('e_regionpol_6C').sum()\n",
    "\n",
    "B1B = df_grouped_area # do not change this variable name\n",
    "\n",
    "df_grouped_area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem B2\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "In the last question, we explored how we could use Pandas to group rows of a dataframe according to a variable's value, and to handle a subset of the remaining columns according to some kind of aggregation logic (such as adding the values or averaging over them). This time, rather than lumping countries together by region, we're going to drill deeper on how an individual nation has changed over time. For this exercise, we're going to look at how democratic norms in Costa Rica have developed in the decades since the Second World War. Since we already have the full dataframe stored in memory (as 'df'), we'll start by filtering our dataset to include only those rows pertaining to Costa Rica. \n",
    "</div>\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "If you examine the resulting dataframe, you might notice that Costa Rica does not have any scores for the 5 democratic indices the earlier years for which it is present in the dataset. This should come as no surprise; even for a group as capable as the VDEM project, constructing a democratic index for the year 1839 would involve enough guesswork to render the result meaningless. As such, we're going to immediately filter our Costa Rica-only dataframe to weed out any rows that don't have scores for the 5 democratic indices. \n",
    "</div>\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "In the following cell block, we're going to find the first year for which we have a complete set of the democratic indices for Costa Rica. Fill in the blanks to continue.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#B2A\n",
    "df_cr = df.query(\"country_name == 'Costa Rica'\")\n",
    "\n",
    "df_cr_filtered = df_cr.dropna(subset=[\n",
    "    'v2x_polyarchy',\n",
    "    'v2x_libdem',\n",
    "    'v2x_partipdem',\n",
    "    'v2x_delibdem',\n",
    "    'v2x_egaldem'])\n",
    "\n",
    "first_year = _____(df_cr_filtered['year'])\n",
    "\n",
    "B2A = first_year # do not change this variable name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Now our data is ready to be plotted! In this part of the exercise, we're going to plot two of Costa Rica's democratic indices against the 'year' variable to see how its democratic norms have evolved over time. We'll accomplish this by using Seaborn and taking advantage of the fact that the columns in Pandas Dataframes can be individually 'pulled out' as a Series (which operate similarly to Numpy arrays, for most intents and purposes). In the following code cell, we'll create the plot for you so you can see how it's done and what it should look like. It won't be graded, and there aren't any blanks to fill in.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr_years = df_cr_filtered['year']\n",
    "cr_polyarchy = df_cr_filtered['v2x_polyarchy']\n",
    "\n",
    "figure = plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(x = cr_years, y = cr_polyarchy)\n",
    "figure.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Despite being as simple as can be, that doesn't look half bad! It's always a good idea to label your axes and give the plot a title so that anyone encoutering it for the first time can rapidly determine what the plot represents. \n",
    "</div>\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Re-create the plot from the code cell above, but take the time to add useful, human-readable labels on the x-axis and y-axis, along with a title describing what the plot is. Fill in the blanks to continue. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#B2B\n",
    "figure = plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(x = cr_years, y = cr_polyarchy)\n",
    "plt.ylabel(_____)\n",
    "plt.xlabel(_____)\n",
    "plt.title(_____)\n",
    "figure.show()\n",
    "\n",
    "B2B = figure # do not change this variable name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem B3\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "In this exercise, we're going to work through how to combine multiple pandas dataframes. This will come in handy whenever you want to explore the relationships between variables that come from different datasets, but which can be linked according to some underlying relationship. \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "In exercise B1, we used addition to aggregate the land area of every nation in a politico-geographic region to give us a sense of how large each region was. In this exercise, we're going to turn our attention to the 5 democracy indices. Using addition (which is what we did with area) to aggregate the 5 democracy indices doesn't make as much sense, though: that might lead us to conclude that regions with more countries would be 'more democratic' than those with only a small number of nations. Instead, we'll *average* over these indicators, which will give us a sense of how democratic each region is, taken together. \n",
    "</div>\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "Now, we'll follow a similar approach to how we tackled the area aggregation: we'll start by creating a dataframe that only includes the columns we care about (the region variable and the 5 democratic indices). This code block won't be graded and doesn't contain any blanks for you to fill in, but be sure to run it anyways!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_democracy = df_2015[['v2x_polyarchy',\n",
    "    'v2x_libdem',\n",
    "    'v2x_partipdem',\n",
    "    'v2x_delibdem',\n",
    "    'v2x_egaldem',\n",
    "     'e_regionpol_6C']]\n",
    "\n",
    "\n",
    "df_grouped_democracy = df_democracy.groupby('e_regionpol_6C').mean()\n",
    "\n",
    "df_grouped_democracy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "If you compare the 'df_grouped_democracy' dataframe and the 'df_grouped_area' dataframe, you might notice that the bolded columns on the left are identical. You may recall that the bold column on the left of a dataframe is the 'index', and we can take advantage of its special status to join the two dataframes together. The result will be one dataframe with the same number of rows, but with all 6 of the columns we aggregated: area and the 5 democratic indices.\n",
    "</div>\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "In the following code block, we're going to concatenate `df_grouped_democracy` and `df_grouped`area. Fill in the blanks to continue. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#B3A\n",
    "df_concatenated_rows = pd._____([df_grouped_democracy, df_grouped_area], axis=1)\n",
    "\n",
    "B3A = df_concatenated_rows # do not change this variable name\n",
    "\n",
    "df_concatenated_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "In the above exercise, we combined dataframes along their rows, using the row index to guide how the data was combined. We can do much the same with columns. To demonstrate how, let's return to our Costa Rica dataframe and add another country to it. Since Costa Rica and Nicaragua are geographic neighbours, it makes sense to compare them directly. \n",
    "</div>\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Time to give Nicaragua the same treatment as we did to Costa Rica in problem B2! Once that's done, we're going to concatenate `df_nicaragua_filtered` and `df_concatenated_cols`, column-wise. Fill in the blanks to continue.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#B3B\n",
    "df_nicaragua = df.query(\"country_name == 'Nicaragua'\")\n",
    "\n",
    "df_nicaragua_filtered = df_nicaragua.dropna(subset=[\n",
    "    'v2x_polyarchy',\n",
    "    'v2x_libdem',\n",
    "    'v2x_partipdem',\n",
    "    'v2x_delibdem',\n",
    "    'v2x_egaldem'])\n",
    "\n",
    "df_concatenated_cols = pd._____([df_nicaragua_filtered, _____])\n",
    "\n",
    "B3B = df_concatenated_cols # do not change this variable name\n",
    "\n",
    "df_concatenated_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Now that the data for these two countries has been combined into a single dataframe, we can easily create plots that allow us to compare them. Again, we'll be using the Seaborn package to do our plotting for us. Even though all of our data is lumped together, Seaborn allows us to use the 'hue' variable to differentiate the data we're plotting based on some categorical variable (which, in this case, is the country variable -- it's what differentiates between Costa Rica and Nicaragua). \n",
    "</div>\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "In the code cell below, we'll create a plot that contains separate lines for both Nicaragua's and Costa Rica's polyarchy score by year. We'll also include labels for the x-axis, y-axis, and plot title. Fill in the blanks to continue.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#B3C\n",
    "concat_years = df_concatenated_cols['year']\n",
    "concat_polyarchy = df_concatenated_cols['v2x_polyarchy']\n",
    "concat_country = df_concatenated_cols['country_name']\n",
    "\n",
    "figure = plt.figure(figsize=(10,6))\n",
    "ax = sns.lineplot(x=concat_years,\n",
    "             y=concat_polyarchy,\n",
    "             hue=concat_country\n",
    "            )\n",
    "ax.set(xlabel=_____, ylabel=_____, title=_____)\n",
    "\n",
    "B3C = figure # do not change this variable name\n",
    "\n",
    "figure.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem B4\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "Pandas and Numpy have been built to interoperate with one another smoothly. Pandas Dataframes and multidimensional Numpy arrays can be interoperable (although with a different set of features); the same goes for Pandas Series and unidimensional Numpy arrays. Let's return to the 'df_grouped_democracy' dataframe we made earlier; it will be useful for exploring how numpy handles multidimensional arrays.\n",
    "</div>\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "In this exercise, we're going to turn our Pandas dataframe into a 6-by-5 Numpy array, and convert every number it contains into a whole-number percentage (which we can accomplish by multiplying by 100 and rounding to the nearest whole number). Fill in the blanks to continue.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#B4A\n",
    "arr_dem = np.array(df_grouped_democracy)\n",
    "\n",
    "arr_dem_percent = arr_dem * _____\n",
    "\n",
    "arr_dem_percent_rounded = np.round(arr_dem_percent)\n",
    "\n",
    "B4A = arr_dem_percent_rounded # do not change this variable name\n",
    "\n",
    "arr_dem_percent_rounded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "If you compare the results from the Numpy array we produced with the values in the Pandas dataframe, you'll notice that they're more-or-less a perfect match (differing only by rounding errors). We can also use Numpy to rapidly and simply perform linear algebra calculations. If, for example, we wanted to see how the polyarchy and liberal democracy variables covary with one another across the regions, we can produce a covariance matrix.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "In the code cell below, we're going to use Numpy to create a covariance matrix for polyarchy and liberal democracy. Fill in the blanks to continue. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#B4B\n",
    "arr_polyarchy = arr_dem[:,0]\n",
    "arr_libdem = arr_dem[:,1]\n",
    "\n",
    "dem_cov = np._____(arr_polyarchy, arr_libdem)\n",
    "\n",
    "B4B = dem_cov # do not change this variable name\n",
    "\n",
    "dem_cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUBMIT YOUR ASSIGNMENT!\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "The subsequent code cell is responsible for taking all of your answers, along with this notebook's metadata, saving them as a 'pickle' file (`.pkl`) in the current working directory. The full process is described in detail at the beginning of this notebook. You shouldn't be able to edit or delete the following code cell (and even if you find a way to do so, it's a good idea to leave it alone). **It is absolutely critical that you run this code cell and ensure that it doesn't raise any exceptions/tracebacks/errors**; if it does, it probably means that it can't find one of the variables it is looking for (`student_id`, or each of the question-specific objects), or that one or more of them isn't in the correct format. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "## FINALIZE ASSIGNMENT\n",
    "\n",
    "import pickle as pkl\n",
    "\n",
    "response_dict = {\n",
    "    \"student_id\": student_id,\n",
    "    \"grad_student\": \"UE_undergrad\",\n",
    "    \"module\": 3,\n",
    "    \"responses\": [\n",
    "        A1A, \n",
    "        A1B,\n",
    "        A2A,\n",
    "        A2B,\n",
    "        A3A,\n",
    "        A3A,\n",
    "        A4A,\n",
    "        A4B,\n",
    "        B1A,\n",
    "        B1B,\n",
    "        B2A,\n",
    "        B2B,\n",
    "        B3A,\n",
    "        B3B,\n",
    "        B3C,\n",
    "        B4A,\n",
    "        B4B,\n",
    "    ],\n",
    "    \"code_cells\": In\n",
    "}\n",
    "\n",
    "module_string = f\"module_{response_dict['module']}\"\n",
    "filename = f\"{module_string}__student_{student_id}.pkl\"\n",
    "\n",
    "with open(f\"./{filename}\", 'wb') as stream:\n",
    "    pkl.dump(response_dict, stream)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
