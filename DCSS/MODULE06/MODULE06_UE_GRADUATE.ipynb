{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><font color=\"gray\">DOING COMPUTATIONAL SOCIAL SCIENCE<br>MODULE 6 <strong>PROBLEM SETS</strong></font>\n",
    "\n",
    "# <font color=\"#49699E\" size=40>Latent Factors and Text Analysis</font>\n",
    "This module notebook assignment is organized into two parts. \n",
    "\n",
    "- **[PART A](#SECAEP) (Accompanying Chapter 9, \"Latent Factors, Dimensions, and Classes\")**\n",
    "    - [Exercises and Practice Problems](#SECAEP) (All students)\n",
    "- **[PART B](#SECBEP) (Accompanying Chapter 10, \"Processing Language Data\"**)\n",
    "    - [Exercises and Practice Problems](#SECBEP) (All students)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What You Need to Know Before Getting Started\n",
    "\n",
    "- **You can consult any resources you want when completing these exercises and problems**. Just as it is in the \"real world:\" if you can't figure out how to do something, look it up. My recommendation is that you check the relevant parts of the assigned reading or search for inspiration on [https://stackoverflow.com](https://stackoverflow.com).\n",
    "- **Each problem is worth 1 point**. All problems are equally weighted.\n",
    "- **The information you need for each problem set is provided in the blue and green cells.** General instructions / the problem set preamble are in the blue cells, and instructions for specific problems are in the green cells. **You have to execute all of the code in the problem set, but you are only responsible for entering code into the code cells that immediately follow a green cell**. You will also recognize those cells because they will be incomplete. You need to replace each `____` with the code that will make the cell execute properly.\n",
    "- **The comments in the problem cells contain clues indicating what the following line of code is supposed to do.** Use these comments as a guide when filling in the blanks. \n",
    "- **Each problem cell stores one object named according to the problem (e.g. _09)**. These are not important for you, but we use them to help grade your work efficiently, so **do not delete them or change their names**. If you do, you will lose marks.\n",
    "- **You can ask for help**. If you run into problems, you can reach out to John (john.mclevey@uwaterloo.ca) or Pierson (pbrowne@uwaterloo.ca) for help. You can ask a friend for help if you like, regardless of whether they are enrolled in the course.\n",
    "\n",
    "Finally, remember that you do not need to \"master\" this content before moving on to other course materials, as what is introduced here is reinforced throughout the rest of the course. You will have plenty of time to practice and cement your new knowledge and skills."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Submit Your (Pickled) Assignment! \n",
    "\n",
    "Since we've had to rethink the way we deliver, collect, and evaluate these problem sets, we want to be very clear about what you need to do to properly submit this module notebook assignment. Please read the following explanation of our process so that you understand how this works, and what you need to do.\n",
    "\n",
    "At the very end of this notebook, there is a code cell that will compile all of your answers to every problem in the assignment and save them as a 'pickle' file (`.pkl`) in the current working directory. You can execute that cell as many times as you like. Each time you run it, it will overwrite the old pickle with your updated answers. **Once you've ensured that everything in the notebook is complete and finished to your satisfaction, it's up to you to get the pickle that you just created and upload it to the appropriate Learn dropbox for this module.** The file you are looking for will not exist until you run the cells at the end of the notebook. Once it has been created, it will follow this naming convention: \n",
    "\n",
    "> `module_[number]__student_[your_student_number].pkl`\n",
    "\n",
    "To be very clear, **you need to submit the pickle to Learn**. You do not need to upload the Jupyter Notebook as initially planned. **Just the pickle!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Sure Everything is Good to Go\n",
    "\n",
    "It's generally a good idea to do a 'fresh' run of your entire notebook before you submit your assignment to make sure that everything is working as it should be. You can use the button with the 'Fast-Forward' arrows in the Jupyter toolbar above to restart the kernel (resetting everything to initial conditions) and running every code cell in the notebook, in order. You can also select 'Restart and Run All' from the Kernel dropdown menu. If the entire notebook runs without throwing any errors, you should be good to go!\n",
    "\n",
    "If you're running into issues, make sure that you haven't changed any of the 'answer' variable names we provided you with (e.g., we asked you to store your answer to the first question in a variable called `_01`). If you change an answer's variable name or don't store your answer in that variable, the project won't finalize properly and you won't get proper credit for your work. The same goes for the `student_id` metadata variable we ask you to complete immediately below; if any of those are missing, haven't been filled in properly, or have been renamed, issues may arise during the grading process and you will not receive proper credit. So make sure you enter your student information, and don't delete or change the names of the variables that store your answers to each problem!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTANT: ADD YOUR STUDENT ID NUMBER\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "To evaluate your work, we need you to provide your student number. In the cell below, <strong>replace '12345678' with your student number</strong>. The student_id' variable needs to be an integer, so <strong>do not wrap it in quotes.</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your UWaterloo student ID number\n",
    "student_id = 12345678 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package and Data Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dcss.paths import vdem_path, internet_freedom_path\n",
    "\n",
    "import os\n",
    "from posixpath import join\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "import spacy\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from dcss.plotting import format_axes_commas, custom_seaborn\n",
    "custom_seaborn()\n",
    "\n",
    "\n",
    "years = list(range(2017, 2020))\n",
    "\n",
    "columns = [\n",
    "    'speechtext', \n",
    "    'speakername', \n",
    "    'speakerparty', \n",
    "    'speakerriding', \n",
    "    'speakeroldname']\n",
    "\n",
    "hansard_paths = []\n",
    "\n",
    "for year in years:\n",
    "    for root, dirs, files in os.walk(f\"data/canadian_hansards/lipad/{year}/\"):\n",
    "        for file in files:\n",
    "            hansard_paths.append(join(root, file))\n",
    "    \n",
    "hansards = []\n",
    "\n",
    "for filename in hansard_paths:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0, usecols=columns).dropna(subset=['speechtext', 'speakername', 'speakerparty'])\n",
    "    hansards.append(df)\n",
    "\n",
    "can_df = pd.concat(hansards, axis=0, ignore_index=True)#.sample(replace=False, frac=.1, random_state=23)\n",
    "\n",
    "evs_df = pd.read_stata(\n",
    "    \"data/european_values_study/ZA7500_v4-0-0.dta\",\n",
    "    convert_categoricals=False,\n",
    "    columns = [\n",
    "        # Overview\n",
    "        #'country',\n",
    "        \n",
    "        # Religion and Morale\n",
    "        'v54', # Religious services? - 1=More than Once Per Week, 7=Never\n",
    "        'v64', # How often do you pray? - 1=More than Once Per Week, 7=Never\n",
    "        'v149', # Do you justify: claiming state benefits? - 1=Never, 10=Always\n",
    "        'v150', # Do you justify: cheating on tax? - 1=Never, 10=Always \n",
    "        'v151', # Do you justify: taking soft drugs? - 1=Never, 10=Always \n",
    "        'v152', # Do you justify: taking a bribe? - 1=Never, 10=Always \n",
    "        'v153', # Do you justify: homosexuality? - 1=Never, 10=Always \n",
    "        'v154', # Do you justify: abortion? - 1=Never, 10=Always \n",
    "        'v155', # Do you justify: divorce? - 1=Never, 10=Always \n",
    "        'v156', # Do you justify: euthanasia? - 1=Never, 10=Always \n",
    "        'v157', # Do you justify: suicide? - 1=Never, 10=Always \n",
    "        'v158', # Do you justify: having casual sex? - 1=Never, 10=Always \n",
    "        'v159', # Do you justify: public transit fare evasion? - 1=Never, 10=Always \n",
    "        'v160', # Do you justify: prostitution? - 1=Never, 10=Always \n",
    "        'v161', # Do you justify: artificial insemination? - 1=Never, 10=Always \n",
    "        'v162', # Do you justify: political violence? - 1=Never, 10=Always \n",
    "        'v163', # Do you justify: death penalty? - 1=Never, 10=Always \n",
    "        \n",
    "        # Politics and Society\n",
    "        'v97', # Interested in Politics? - 1=Interested, 4=Not Interested\n",
    "        'v121', # How much confidence in Parliament? - 1=High, 4=Low\n",
    "        'v124', # How much confidence in EU? - 1=High, 4=Low\n",
    "        'v126', # How much confidence in Health Care System? - 1=High, 4=Low\n",
    "        'v142', # Importance of Democracy - 1=Unimportant, 10=Important\n",
    "        'v143', # Democracy in own country - 1=Undemocratic, 10=Democratic\n",
    "        'v145', # Political System: Strong Leader - 1=Good, 4=Bad\n",
    "        'v146', # Political System: Experts Making Decisions - 1=Good, 4=Bad\n",
    "        'v147', # Political System: Army Rules the Country - 1=Good, 4=Bad\n",
    "        'v148', # Political System: Democratic - 1=Good, 4=Bad\n",
    "        'v208', # How often follow politics on TV? - 1=Daily, 5=Never\n",
    "        'v211', # How often follow politics on Social Media? - 1=Daily, 5=Never\n",
    "        \n",
    "        # National Identity\n",
    "        'v170', # How proud are you of being a citizen? - 1=Proud, 4=Not Proud\n",
    "        'v184', # Immigrants: impact on development of country - 1=Bad, 5=Good\n",
    "        'v185', # Immigrants: take away jobs from Nation - 1=Take, 10=Do Not Take\n",
    "        'v198', # European Union Enlargement - 1=Should Go Further, 10=Too Far Already\n",
    "        \n",
    "        # Socio-demographics\n",
    "        'v226', # Year of Birth by respondent \n",
    "        'v227', # Respondent Born in Country - 1=Yes, 2=No\n",
    "        'v261_ppp', # Household Monthly Net Income, PPP-Corrected\n",
    "    ])\n",
    "\n",
    "just_country = pd.read_stata(\n",
    "    \"../../data/european_values_study/ZA7500_v4-0-0.dta\",\n",
    "    columns = ['country'],\n",
    ")\n",
    "\n",
    "evs_df[evs_df < 0] = np.nan\n",
    "\n",
    "evs_df['country'] = list(just_country['country'])\n",
    "\n",
    "evs_df = evs_df.dropna()\n",
    "\n",
    "country_index = evs_df['country'].to_numpy()\n",
    "\n",
    "print(evs_df['country'].value_counts())\n",
    "\n",
    "evs_df = evs_df.drop(\"country\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"#49699E\">PART A | CHAPTER 9</font>\n",
    "<a id=\"SECAEP\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1:\n",
    "<div class=\"alert alert-block alert-info\">  \n",
    "In Part A of this assignment, we're going to be working with data from the European Values Survey (EVS). The data is comprised of data collected by interviewers, and was drawn from most European nations in 2017. In the (intimidatingly large) import cell above, we took the liberty of loading the data for you and removing all of the rows contianing missing data. There's also a breakdown of how many responses come from each country.\n",
    "</div>\n",
    "<div class=\"alert alert-block alert-info\">  \n",
    "As established in the first of the two chapters you read this week, standardized data is a <i>sine qua non</i> when working with latent variables!\n",
    "</div>\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Standardize the data present in the columns of the <code>evs_df</code> variable. Submit the standardized data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_01\n",
    "\n",
    "# Standardize the data\n",
    "X = _____()._____(_____)\n",
    "\n",
    "print(X)\n",
    "\n",
    "# Store the result in the assignment variable\n",
    "_01 = X # do not change this variable name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2:\n",
    "<div class=\"alert alert-block alert-info\">  \n",
    "Just like in the chapter, we'll proceed with our now-standardized by producing a principal components analysis. Unlike in the chapter, however, we're only going to have our PCA return the top 10 components.\n",
    "</div>\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Perform a principal components analysis on the standardizd EVS data. Only return the top 10 components (sorted in order of explained variance ratio). Submit a numpy array containing the explained variance ratios of the 10 principal components you found. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_02\n",
    "\n",
    "# Create PCA with 10 components\n",
    "pca = _____(_____, random_state=42)\n",
    "\n",
    "# Fit the PCA\n",
    "pca._____(X)\n",
    "\n",
    "# Extract explained variance ratio\n",
    "evr = pca._____\n",
    "\n",
    "print(evr)\n",
    "\n",
    "# Store the result in the assignment variable\n",
    "_02 = evr # do not change this variable name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3:\n",
    "<div class=\"alert alert-block alert-info\">  \n",
    "Although the process of interpreting screeplots is generally subjective and open to interpretation, we're fortunate in that the screeplot from our PCA of the EVS data has a clear inflection point. It's time to flex your interpretation muscles! \n",
    "</div>\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Produce a screeplot of the 10 principal components you produced in question 2. Submit an integer corresponding to the principal component ID that corresponds with the inflection point.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_03\n",
    "\n",
    "# Extract explained variances\n",
    "eigenvalues = pd.Series(_____._____)\n",
    "\n",
    "# Create screeplot\n",
    "fig, ax = plt._____()\n",
    "sns.lineplot(x=eigenvalues.index, y=eigenvalues, data=eigenvalues)\n",
    "plt.scatter(x=eigenvalues.index, y=eigenvalues)\n",
    "ax.set(xlabel='Principal component ID', ylabel='Eigenvalue')\n",
    "sns.despine()\n",
    "plt.show()\n",
    "\n",
    "inflection_point = _____\n",
    "\n",
    "# Store the result in the assignment variable\n",
    "_03 = inflection_point # do not change this variable name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4:\n",
    "<div class=\"alert alert-block alert-info\">  \n",
    "In the following code cell, we're going to ask you to perform a K-means cluster with a K of 10, AND to write a function capable of telling us a bit more about the nationalities of the individuals from each cluster. That's a lot of code for you to write, but don't be intimidated: you can treat this as two separate problems that you need to solve sequentially (consider commenting out all of the hint code regarding the function while you're working on creating the K-means analysis).\n",
    "</div>\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Perform a K-means cluster analysis, where K = 10, on the EVS data. Store the labels from your K-means cluster analysis in the 'cluster' variable (provided for you). Then, write a function that zips together your list of cluster assignments and the list of countries from the EVS data (both should be the same length), and then iterates over this zipped list in order to create a dictionary where each key is a cluster number (0 through 9) and each value is a list of the 5 nationalities that most frequently appear in that cluster. We have provided a screenshot of what the result should look like (based on the pprint function call we've given you). Submit what your function returns . \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_04\n",
    "\n",
    "# Set number of clusters\n",
    "num_clusters = 10\n",
    "\n",
    "# Instantiate k-means \n",
    "km = _____(n_clusters=num_clusters, init='k-means++', random_state=42)\n",
    "\n",
    "# Fit instantiated k-means to data\n",
    "k_means_fitted = km._____(X)\n",
    "\n",
    "# Extract clusters\n",
    "clusters = k_means_fitted.labels_.tolist() # Do not change this line\n",
    "\n",
    "# Write function for summarizing results of k-means clustering\n",
    "def top_countries_by_cluster(\n",
    "    km, \n",
    "    num_clusters, \n",
    "    country_index, \n",
    "    return_top = 5):\n",
    "\n",
    "    # Create zipped list of k-means labels and the country column from original dataset\n",
    "    cluster_countries = _____(_____(km.labels_, country_index))\n",
    "    \n",
    "    # Initialize cluster dictionary\n",
    "    cluster_count = {i:{} for i in _____(num_clusters)}\n",
    "\n",
    "    # Iterate over cluster-country pairs:\n",
    "    for pair _____ _____:\n",
    "        \n",
    "        # Extract cluster and country\n",
    "        cluster_num = pair[0]\n",
    "        country = pair[1]\n",
    "\n",
    "        # Retrieve current count of particular nationality in cluster\n",
    "        current_count = cluster_count[cluster_num].get(_____, 0)\n",
    "        \n",
    "        # Increment count by one \n",
    "        current_count _____ 1\n",
    "        \n",
    "        # Store incremented count as new value \n",
    "        cluster_count[cluster_num][country] = current_count\n",
    "    \n",
    "    # Sort the values of the cluster dictionary and filter down to 5 most common nationalities for each cluster\n",
    "    for cluster_num, country_dict in cluster_count.items():\n",
    "        cluster_count[cluster_num] = _____(country_dict, key=lambda x: country_dict[x], reverse=True)[0:return_top]\n",
    "\n",
    "    # Return final count dictionary\n",
    "    return _____\n",
    "\n",
    "        \n",
    "pprint(top_countries_by_cluster(k_means_fitted, num_clusters, country_index))\n",
    "    \n",
    "    \n",
    "# Store the result in the assignment variable\n",
    "_04 = top_countries_by_cluster(k_means_fitted, num_clusters, country_index) # do not change this variable name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5:\n",
    "<div class=\"alert alert-block alert-info\">  \n",
    "In question 4, you familiarized yourself with K-means clustering and built a function capable of summarizing some of your results. Of course, the number of clusters we used above was arbitrary, and might be nowhere close to the optimal number of clusters needed for the data. In this question, we're going to use silhouette analysis to determine a good number of centroids to cluster the data around. It's highly likely that our silhouette analysis will indicate that 2 or 3 clusters is optimal, but since we want to tease out regional variations, we'll insist on finding an optimal solution using more than 4 clusters.\n",
    "</div>\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Using silhouette analysis, find an optimal number of clusters for the EVS data that's greater than 4 and less than 11. Submit the K value you have decided to use (as an integer). If the silhouette analysis was conducted properly, there should be a clear best option.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_05\n",
    "\n",
    "# Iterate over appropriate range of clusters\n",
    "for i in range(_____, _____):\n",
    "    \n",
    "    # Extract value of iterator for consistency \n",
    "    num_clusters = i\n",
    "\n",
    "    # Run k-means with iterated number of clusters (see question 4 for detailed breakdown)\n",
    "    km = _____(n_clusters=_____, init='k-means++', random_state=42)\n",
    "    k_means_fitted = km._____(X)\n",
    "    clusters = k_means_fitted.labels_.tolist() # Do not change this line\n",
    "    \n",
    "    # Print cluster number\n",
    "    print(f\"{i} clusters:\")\n",
    "\n",
    "    # Print silhouette score\n",
    "    print(_____(X, _____, metric='euclidean'))\n",
    "\n",
    "    # Extract silhouette samples\n",
    "    samples = _____(X, _____)\n",
    "\n",
    "    # Plot extracted samples\n",
    "    ax = sns.displot(samples, kind=\"ecdf\")\n",
    "    ax.set(xlabel='Silhouette Score')\n",
    "    sns.despine()\n",
    "    plt.xlim(-1, 1)\n",
    "    plt.axvline(x=0, linewidth=2, color='darkgray')\n",
    "    plt.show()\n",
    "\n",
    "# Provide integer value corresponding to the number of clusters you have decided to use\n",
    "final_number_of_clusters = _____\n",
    "    \n",
    "# Store the result in the assignment variable\n",
    "_05 = final_number_of_clusters # do not change this variable name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6:\n",
    "<div class=\"alert alert-block alert-info\">  \n",
    "Now that we've used silhouette to determine the optimal number of clusters from within a given range, we'll re-perform the cluster analysis and use the function you wrote in question 4 to summarize the results! Then, it's up to you to write a two-sentence analysis of the results. Do you notice any problems or peculiarities with the results? If you do (and you should), write a further two sentences describing the issue(s) you noticed and how you might go about fixing them. \n",
    "</div>\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Perform a K-means analysis of the EVS data using the K value determined in question 5. Then, use your function from question 4 to extract the 5 countries which appear most frequently in each cluster. Write two sentences inside the docstring analyzing the results. Write another two sentences detailing obvious improvements that could be implemented to increase the quality of the results (examine the results of the `value_counts()` method performed on the 'country' column; you can find this in the final cell of the 'Package and Data Imports' section above).  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_06\n",
    "\n",
    "# Extract number of clusters from previous question and store in new variable for consistency\n",
    "num_clusters = final_number_of_clusters\n",
    "\n",
    "# Perform k-means with selected number of clusters\n",
    "km = _____(n_clusters=_____, init='k-means++', random_state=42)\n",
    "k_means_fitted = km._____(X)\n",
    "clusters = k_means_fitted.labels_.tolist() # Do not change this line\n",
    "\n",
    "# Print results\n",
    "pprint(top_countries_by_cluster(k_means_fitted, num_clusters, country_index))\n",
    "    \n",
    "analysis = \"\"\"\n",
    "\n",
    "SENTENCE ONE OF YOUR ANALYSIS GOES HERE.\n",
    "SENTENCE TWO OF YOUR ANALYSIS GOES HERE.\n",
    "\n",
    "SENTENCE ONE OF YOUR SUGGESTIONS FOR IMPROVING RESULTS GOES HERE.\n",
    "SENTENCE TWO OF YOUR SUGGESTIONS FOR IMPROVING RESULTS GOES HERE.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Store the result in the assignment variable\n",
    "_06 = analysis # do not change this variable name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"#49699E\">PART B | CHAPTER 10</font>\n",
    "<a id=\"SECAEP\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7:\n",
    "<div class=\"alert alert-block alert-info\">  \n",
    "For this second half of the assignment, we're going to turn our attention to the Canadian Hansards. To save you the trouble of having to work with the entire set of Hansards from 2017 through to 2020, we're going to filter them down to only include speeches by federal party leaders present in the House of Commons. Once that's done, we'll run the entire set of speeches through spaCy's nlp pipe, which will prepare us for the subsequent sections!\n",
    "</div>\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Using the provided list of filter keywords, filter the Canadian Hansard dataframe so that each row consists of a speech by one of the leaders of the Federal Political Parties over the last two years. Put the text of the speeches through spaCy's nlp pipeline using the .pipe() method. Submit the part of speech of the first word of the speech at index 250 of the resulting list of processed speeches.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_07\n",
    "\n",
    "filter_terms = [\n",
    "    \"Justin Trudeau\", # Prime Minister, 2015-Present; Leader of the Liberal Party of Canada, 2013-Present\n",
    "    \"Andrew Scheer\", # Leader of the Official Opposition, 2017-2019\n",
    "    \"Mario Beaulieu\", # Interim Leader of the Bloc Quebequois, 2018-2019\n",
    "    \"Jagmeet Singh\", # Leader of the New Democratic Party, 2017-Present\n",
    "    \"Elizabeth May\", # Leader of the Green Party of Canada, 2006-2019\n",
    "    \"PPC\", # Maxime Bernier in his capacity as Leader of the People's Party of Canada, 2018-Present \n",
    "]\n",
    "\n",
    "# Instantiate list for dataset subsets\n",
    "leader_speeches = []\n",
    "\n",
    "# Iterate over filter_terms, filter dataset to term, and store resulting dataframe subset in 'leader_speeches'\n",
    "for t in _____:\n",
    "    h = can_df[_____[\"speakeroldname\"].str.contains(_____, na=False)]\n",
    "    leader_speeches._____(h)\n",
    "\n",
    "# Concatenate dataframes in 'leader_speeches'\n",
    "leader_df = pd._____(leader_speeches, axis=0, ignore_index=True)\n",
    "\n",
    "# Initialize nlp pipeline, taking care to disable named entity recognition\n",
    "nlp = _____._____('en_core_web_sm', disable=[\"ner\"])\n",
    "\n",
    "# Use list comprehension to feed each speech through spaCy's nlp pipe\n",
    "corpus = [s for s in _____.pipe(leader_df['speechtext'])] \n",
    "\n",
    "# Extract part of speech from the first word of the speech at index 250\n",
    "speech_250_pos = corpus[_____][_____]._____\n",
    "\n",
    "pprint(speech_250_pos)\n",
    "\n",
    "# Store the result in the assignment variable\n",
    "_07 = speech_250_pos # do not change this variable name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8:\n",
    "<div class=\"alert alert-block alert-info\">  \n",
    "Now that we have our output from spaCy, we have access to a wealth of information about each word in our dataset. We'll start by using this information to create a list of lists, where the outer list contains a number of inner lists, and each of the inner lists represents a single speech. We'll populate those inner lists with the lemmatized forms of all the nouns and proper nouns present in their corresponding speeches.   \n",
    "</div>\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Iterate over the docs to create a list of lists, where the outer list represents the documents and the inner list contains a list of lemmatized nouns and proper nouns. Submit the list of lemmatized nouns and proper nouns associated with the speech located at index 500 of the resulting list of speeches.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_08\n",
    "\n",
    "# Populate list containing desired parts of speech\n",
    "filter_list = [\"_____\", \"_____\"]\n",
    "\n",
    "# Initialize list for containing results\n",
    "lemma_list = []\n",
    "\n",
    "# Iterate over speeches\n",
    "_____ speech _____ corpus:\n",
    "    # Iterate over each word in speech, add the lemmatized word if it matches one of the desired parts of speech\n",
    "    lemma_list._____([n._____ for n in speech if n._____ in _____])\n",
    "\n",
    "    \n",
    "# Extract speech from corpus at index 500\n",
    "speech_500 = lemma_list[_____]\n",
    "\n",
    "pprint(speech_500)\n",
    "    \n",
    "# Store the result in the assignment variable\n",
    "_08 = speech_500 # do not change this variable name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9:\n",
    "<div class=\"alert alert-block alert-info\">  \n",
    "The solution to this question is going to be very similar to that of the previous question. The major difference here is that we're going to add two new types of token to our inner lists: verbs and adjectives.\n",
    "</div>\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Iterate over the docs to create a list, where the outer list represents the documents and the inner list contains a list of lemmatized nouns, proper nouns, verbs, and adjectives. Submit the list of lemmatized nouns, proper nouns, verbs, and adjectives associated with the speech located at index 750 of the resulting list of speeches.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_09\n",
    "\n",
    "# Populate list containing desired parts of speech\n",
    "filter_list = [\"_____\", \"_____\", \"_____\", \"_____\"]\n",
    "\n",
    "# Initialize list for containing results\n",
    "lemma_list_2 = []\n",
    "\n",
    "# Iterate over speeches\n",
    "_____ speech _____ corpus:\n",
    "    # Iterate over each word in speech, add the lemmatized word if it matches one of the desired parts of speech\n",
    "    lemma_list_2._____([n._____ for n in speech if n._____ in _____])\n",
    "\n",
    "speech_750 = lemma_list_2[_____]\n",
    "    \n",
    "pprint(speech_750)\n",
    "\n",
    "# Store the result in the assignment variable\n",
    "_09 = speech_750 # do not change this variable name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 10:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">  \n",
    "One of the more useful parsing options spaCy provides is robust sentence detection, which is often useful for downstream tasks and sometimes even required. One of those tasks is the bigram detection implementation in gensim. Because bigrams are usually common phrases, they often end up being some kind of topic, even if it's not the core topic of the text. In these next problems, you will create a list of the most frequently occurring bigrams for each political leader. \n",
    "</div>\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Using the corpus object you created earlier, create a flat list of all of the tokenized sentences in order to train a gensim Phrases model. While you're at it, use the leader_speeches list of dataframes to prepare a nested list of tokenized speech sentences for each political leader, which you will apply the trained bigram model on. Submit the first sentence by the first leader.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_10\n",
    "\n",
    "# Initialize list for storing results\n",
    "sent_list = []\n",
    "\n",
    "# Iterate over speeches in corpus\n",
    "for _____ in _____:\n",
    "    # iterate over the sentences in each speech, and then the tokens from each of those sentences...\n",
    "    # ... add each token to 'sent_list' *as individual words, not lists*.\n",
    "    sent_list._____([[token.text for token in sent] for sent in speech.sents])\n",
    "\n",
    "# Initialize list for storing results\n",
    "leader_sent_lists = []\n",
    "\n",
    "# Iterate over the separate leader-specific dataframes in the 'leader_speeches' variable (created in question 7)\n",
    "_____ df _____ leader_speeches:\n",
    "    \n",
    "    # Initialize list for storing results within the loop \n",
    "    leader_sents = []\n",
    "    \n",
    "    # Run the individual dataframe (from the loop) through spacy's nlp pipe and iterate over the results\n",
    "    for speech in _____._____(df['speechtext']):\n",
    "        \n",
    "        # iterate over the sentences in the speech, and then the tokens from each of those sentences...\n",
    "        # ... add each token to 'sent_list' *as individual words, not lists*.\n",
    "        leader_sents._____([[token.text for token in sent] _____ sent _____ speech.sents])\n",
    "        \n",
    "    # Add the 'leader_sents' list to 'leader_sent_lists' *as a list*\n",
    "    leader_sent_lists._____(leader_sents)\n",
    "\n",
    "# Retrieve the first sentence spoken by the first leader in the list of lists.\n",
    "first_sent = leader_sent_lists[_____][_____]\n",
    "\n",
    "pprint(first_sent)\n",
    "\n",
    "# Store the result in the assignment variable\n",
    "_10 = first_sent # do not change this variable name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 11:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">  \n",
    "Now we can train a model and use it on the text for each leader!\n",
    "</div>\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Using the sentences in the corpus, train a gensim Phrases model. Apply that model to the list that contains the list of sentences for each leader. Submit the bigrammed first sentence of the first speech.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_11\n",
    "\n",
    "# Train the model using the data from Question 10\n",
    "model = Phrases(sent_list, min_count=1, threshold=0.75,\n",
    "                    scoring='npmi')  # train the model\n",
    "    \n",
    "# Create the model applicator\n",
    "bigrammer = _____(model)  \n",
    "\n",
    "# Initialize list\n",
    "bigrammed_list = []\n",
    "\n",
    "# Iterate over the lists in the leader_sent_lists object\n",
    "_____ sent_list _____ leader_sent_lists:\n",
    "    \n",
    "    # Initialize in-loop list\n",
    "    bigrammed_sents = []\n",
    "    \n",
    "    # Iterate over each sentence in 'sent_list'\n",
    "    _____ sent _____ sent_list:\n",
    "        \n",
    "        # Subscript the bigrammer with the sentence and store result \n",
    "        bigrammed_sent = _____[sent]\n",
    "        \n",
    "        # Add the bigrammed sentence to list of bigrammed sentences\n",
    "        bigrammed_sents._____(bigrammed_sent)\n",
    "    \n",
    "    # Add list of bigrammed sentences from leader to 'bigrammed_list'\n",
    "    bigrammed_list._____(bigrammed_sents)\n",
    "    \n",
    "# Extract first sentence from first leader in list of lists ('bigrammed_list') \n",
    "first_sent = _____[_____][_____]\n",
    "\n",
    "pprint(first_sent)\n",
    "\n",
    "# Store the result in the assignment variable\n",
    "_11 = first_sent # do not change this variable name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 12:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">  \n",
    "Given that the speeches in your list of lists should - if everything went according to plan - appear in the same order as they did in the list of dataframes we used at the beginning of this part, you should be able to match up the list of bigrammed sentences to tell who's doing the talking. \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Print the 10 most common bigrams for each leader using a Pandas series. Hint: each leader has a list of tokenized sentences, where each sentence is a list of tokens and bigram tokens are two words joined by \"_\". Submit the name of the leader that talked about Donald Trump a lot.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_12\n",
    "\n",
    "# Zip together 'filter_terms' and 'bigrammed_list', and iterate over them\n",
    "for leader, sent_list in _____(_____, _____):\n",
    "    \n",
    "    # Initialize list\n",
    "    bigrams = []\n",
    "    \n",
    "    # Iterate over sentences in sentence list (from bigrammed_list)\n",
    "    _____ sent _____ sent_list:    \n",
    "        \n",
    "        # Iterate over tokens in sentence and extract bigrams...\n",
    "        # ... which can be identified by the presence of an underscore '_' ...\n",
    "        # ... and add them to the 'bigrams' list *as a list* \n",
    "        bigrams._____([token _____ token _____ sent if '_' in token])\n",
    "        \n",
    "    # Convert list of bigrams into a pandas series\n",
    "    bigram_series = _____._____(bigrams)\n",
    "    \n",
    "    # Print the leader's top ten most-spoken bigrams\n",
    "    print(leader + '\\n')\n",
    "    print(bigram_series._____()[:_____])\n",
    "    print('\\n')\n",
    "\n",
    "# Write down the name of the leader who talks about Donald Trump with disproportionate frequency! \n",
    "trump_talker = '_____ _____'\n",
    "\n",
    "# Store the result in the assignment variable\n",
    "_12 = trump_talker.upper() # do not change this variable name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FINALIZE ASSIGNMENT\n",
    "\n",
    "import pickle as pkl\n",
    "\n",
    "module = 6\n",
    "\n",
    "response_dict = {\n",
    "    \"student_id\": student_id,\n",
    "    \"grad_student\": \"UE_grad\",\n",
    "    \"module\": module,\n",
    "    \"responses\": [\n",
    "        _01,\n",
    "        _02,\n",
    "        _03,\n",
    "        _04,\n",
    "        _05,\n",
    "        _06,\n",
    "        _07,\n",
    "        _08,\n",
    "        _09,\n",
    "        _10,\n",
    "        _11,\n",
    "        _12,\n",
    "    ],\n",
    "    \"code_cells\": In\n",
    "}\n",
    "\n",
    "module_string = f\"module_{response_dict['module']}\"\n",
    "filename = f\"{module_string}__student_{student_id}.pkl\"\n",
    "\n",
    "with open(f\"./{filename}\", 'wb') as stream:\n",
    "    pkl.dump(response_dict, stream)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dcss",
   "language": "python",
   "name": "dcss"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
