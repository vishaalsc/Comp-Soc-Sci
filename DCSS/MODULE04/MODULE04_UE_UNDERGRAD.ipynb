{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><font color=\"gray\">DOING COMPUTATIONAL SOCIAL SCIENCE<br>MODULE _ <strong>PROBLEM SETS</strong></font>\n",
    "\n",
    "# <font color=\"#49699E\" size=40>MODULE TEMPLATE</font>\n",
    "This module notebook assignment is organized into two parts. \n",
    "\n",
    "- **[PART A](#SECAEP) (Accompanying Chapter _, \"_____\")**\n",
    "    - [Exercises and Practice Problems](#SECAEP) (All students)\n",
    "- **[PART B](#SECBEP) (Accompanying Chapter _, \"_____\"**)\n",
    "    - [Exercises and Practice Problems](#SECBEP) (All students)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What You Need to Know Before Getting Started\n",
    "\n",
    "- **You can consult any resources you want when completing these exercises and problems**. Just as it is in the \"real world:\" if you can't figure out how to do something, look it up. My recommendation is that you check the relevant parts of the assigned reading or search for inspiration on [https://stackoverflow.com](https://stackoverflow.com).\n",
    "- **Each problem is worth 1 point**. All problems are equally weighted.\n",
    "- **The information you need for each problem set is provided in the blue and green cells.** General instructions / the problem set preamble are in the blue cells, and instructions for specific problems are in the green cells. **You have to execute all of the code in the problem set, but you are only responsible for entering code into the code cells that immediately follow a green cell**. You will also recognize those cells because they will be incomplete. You need to replace each `____` with the code that will make the cell execute properly.\n",
    "- **The comments in the problem cells contain clues indicating what the following line of code is supposed to do.** Use these comments as a guide when filling in the blanks. \n",
    "- **Each problem cell stores one object named according to the problem (e.g. _09)**. These are not important for you, but we use them to help grade your work efficiently, so **do not delete them or change their names**. If you do, you will lose marks.\n",
    "- **You can ask for help**. If you run into problems, you can reach out to John (john.mclevey@uwaterloo.ca) or Pierson (pbrowne@uwaterloo.ca) for help. You can ask a friend for help if you like, regardless of whether they are enrolled in the course.\n",
    "\n",
    "Finally, remember that you do not need to \"master\" this content before moving on to other course materials, as what is introduced here is reinforced throughout the rest of the course. You will have plenty of time to practice and cement your new knowledge and skills."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Submit Your (Pickled) Assignment! \n",
    "\n",
    "Since we've had to rethink the way we deliver, collect, and evaluate these problem sets, we want to be very clear about what you need to do to properly submit this module notebook assignment. Please read the following explanation of our process so that you understand how this works, and what you need to do.\n",
    "\n",
    "At the very end of this notebook, there is a code cell that will compile all of your answers to every problem in the assignment and save them as a 'pickle' file (`.pkl`) in the current working directory. You can execute that cell as many times as you like. Each time you run it, it will overwrite the old pickle with your updated answers. **Once you've ensured that everything in the notebook is complete and finished to your satisfaction, it's up to you to get the pickle that you just created and upload it to the appropriate Learn dropbox for this module.** The file you are looking for will not exist until you run the cells at the end of the notebook. Once it has been created, it will follow this naming convention: \n",
    "\n",
    "> `module_[number]__student_[your_student_number].pkl`\n",
    "\n",
    "To be very clear, **you need to submit the pickle to Learn**. You do not need to upload the Jupyter Notebook as initially planned. **Just the pickle!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Sure Everything is Good to Go\n",
    "\n",
    "It's generally a good idea to do a 'fresh' run of your entire notebook before you submit your assignment to make sure that everything is working as it should be. You can use the button with the 'Fast-Forward' arrows in the Jupyter toolbar above to restart the kernel (resetting everything to initial conditions) and running every code cell in the notebook, in order. You can also select 'Restart and Run All' from the Kernel dropdown menu. If the entire notebook runs without throwing any errors, you should be good to go!\n",
    "\n",
    "If you're running into issues, make sure that you haven't changed any of the 'answer' variable names we provided you with (e.g., we asked you to store your answer to the first question in a variable called `_01`). If you change an answer's variable name or don't store your answer in that variable, the project won't finalize properly and you won't get proper credit for your work. The same goes for the `student_id` metadata variable we ask you to complete immediately below; if any of those are missing, haven't been filled in properly, or have been renamed, issues may arise during the grading process and you will not receive proper credit. So make sure you enter your student information, and don't delete or change the names of the variables that store your answers to each problem!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTANT: ADD YOUR STUDENT ID NUMBER\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "To evaluate your work, we need you to provide your student number. In the cell below, <strong>replace '12345678' with your student number</strong>. The student_id' variable needs to be an integer, so <strong>do not wrap it in quotes.</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your UWaterloo student ID number\n",
    "student_id = 12345678 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from IPython.core.display import display, HTML\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"#49699E\">PART A | CHAPTER 6 - APIs</font>\n",
    "<a id=\"SECAEP\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1:\n",
    "<div class=\"alert alert-block alert-info\">  \n",
    "Wading into endlessly nested JSON structures and sorting through text that's full of HTML tags can be a bit daunting. Thankfully, the well-structured nature of API responses means that more straightforward numeric data is usually easily accessed. Before starting, make sure your `cred.py` is properly setup like the example in the chapter.\n",
    "</div>\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "In this exercise, you will search the Guardian API for the term \"morrison\" between the dates of 2019-05-18 and 2019-05-19 - the day Australian Prime Minister Scott Morrison won an election and the day after. Using a loop, search for the term in each of the three production offices ('aus', 'uk', and 'us') and store each result in a list. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_01\n",
    "\n",
    "import cred\n",
    "\n",
    "# API key provided by the Guardian\n",
    "GUARDIAN_KEY = cred.GUARDIAN_KEY\n",
    "\n",
    "# Initialize Constants:\n",
    "API_ENDPOINT = 'http://content.guardianapis.com/search' \n",
    "office_list = ['aus','uk','us']\n",
    "\n",
    "# Set up the request parameters, including authorization\n",
    "PARAMS = {'api-key': GUARDIAN_KEY,                \n",
    "             'from-date': '2019-05-18',\n",
    "             'to-date': '2019-05-19',\n",
    "             'q': 'morrison'}\n",
    "\n",
    "# Initialize list for storing results from iteration\n",
    "morrison_list = []\n",
    "\n",
    "# Iterate over each of the offices in 'office list'\n",
    "for office in office_list:\n",
    "    \n",
    "    PARAMS['production-office'] = office      # set the search filter on each pass of the loop\n",
    "    \n",
    "    response = requests._____(API_ENDPOINT, params=PARAMS)    # send the query to the Guardian API\n",
    "    response_dict = response.json()['response']         # keep the relevant component of the response\n",
    "    \n",
    "    total_articles = response_dict['total']        # access the metafield that indicates the total number of responses\n",
    "    morrison_list._____(total_articles)           # add the resulting data to the list of results\n",
    "    \n",
    "    print(office + ': ' + str(total_articles))        # print results\n",
    "    \n",
    "# Store the result in the assignment variable\n",
    "_01 = morrison_list # do not change this variable name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2:\n",
    "<div class=\"alert alert-block alert-info\">  \n",
    "Next we'll grab another search term - this time 'trump' - for the same time period and the same set of three production offices. \n",
    "</div>\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Query the Guardian API for 'trump' and store the results in a list much like last time. Create a pandas dataframe out of all three lists, giving each column a reasonably descriptive name. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_02\n",
    "\n",
    "# Initialize list for storing results from iteration\n",
    "trump_list = []\n",
    "\n",
    "#Modify the search term\n",
    "PARAMS['q'] = 'trump'                  \n",
    "\n",
    "# Iterate over each of the offices in 'office list'\n",
    "for _____ in _____:\n",
    "    \n",
    "    PARAMS['production-office'] = office # set the search filter on each pass of the loop\n",
    "    \n",
    "    response = _____.get(API_ENDPOINT, params=PARAMS)  # send the query to the Guardian API\n",
    "    response_dict = response.json()['response'] # keep the relevant component of the response\n",
    "    \n",
    "    total_articles = response_dict['total'] # access the metafield that indicates the total number of responses\n",
    "    trump_list.append(total_articles)       # add the resulting data to the list of results\n",
    "\n",
    "# Initialize a dataframe and create columns from the lists\n",
    "df = pd.DataFrame()\n",
    "df['office'], df['morrison'], df['trump'] = office_list, morrison_list, trump_list       \n",
    "\n",
    "# Store the result in the assignment variable\n",
    "_02 = df # do not change this variable name\n",
    "\n",
    "# See the whole (small) dataframe\n",
    "df.head()              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3:\n",
    "<div class=\"alert alert-block alert-info\">  \n",
    "Let's take a look at what kind of stories from the UK office were mentioning Trump in that time period.\n",
    "</div>\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Reconfigure the appropriate `PARAMS` dictionary entries to carry out the search, adding 'headline' to the request. Retrieve the headlines from each article returned in the response, store them in a list, and take a look at the topics suggested by the headlines! \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_03\n",
    "\n",
    "# Initialize list for storing results from iteration\n",
    "trump_uk_headlines = []\n",
    "\n",
    "# Change some of the request parameters again\n",
    "PARAMS['q'] = 'trump'                          \n",
    "PARAMS['production-office'] = 'uk'\n",
    "PARAMS['show-fields'] = 'headline'\n",
    "\n",
    "response = requests.get(_____, params=_____)  # send the query to the Guardian API\n",
    "response_dict = response.json()['response']   # keep the relevant component of the response\n",
    "\n",
    "# Iterate over each of the responses\n",
    "for resp in response_dict['results']:\n",
    "    headline = resp['fields']['headline']        # process the new result field\n",
    "    trump_uk_headlines._____(headline)          # add the resulting data to the list of results\n",
    "    \n",
    "# Store the result in the assignment variable\n",
    "_03 = trump_uk_headlines # do not change this variable name\n",
    "\n",
    "# View the list you just finished making\n",
    "pprint(trump_uk_headlines) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4:\n",
    "<div class=\"alert alert-block alert-info\">  \n",
    "Word counts are also often at least a byproduct of many API processes, so they will often be available. Let's see if there was much difference in the average word count of articles mentioning Trump, across all production offices, compared to those mentioning Morrison.\n",
    "</div>\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Modify the relevant entries in `PARAMS`, retrieving the wordcount for each article in the response text. Results should be stored in a list for each politician to calculate and print the average word counts.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_04\n",
    "\n",
    "# Initialize lists for storing results from iteration\n",
    "morrison_counts = []\n",
    "trump_counts = []\n",
    "\n",
    "# Need to request a new field and clear the office filter\n",
    "PARAMS['q'] = 'morrison'\n",
    "PARAMS['production-office'] = None\n",
    "PARAMS['show-fields'] = 'wordcount'             \n",
    "\n",
    "response = requests.get(API_ENDPOINT, params=PARAMS)   # send the query to the Guardian API\n",
    "response_dict = response.json()['response']            # keep the relevant component of the response \n",
    "\n",
    "# Fill the list of words counts for the first search\n",
    "for resp in response_dict['results']:\n",
    "    wordcount = resp['fields']['wordcount']       # retrieve the word count\n",
    "    morrison_counts._____(int(wordcount))        # int() is needed because the API returns strings\n",
    "\n",
    "    \n",
    "PARAMS['q'] = 'trump'\n",
    "response = requests.get(API_ENDPOINT, params=PARAMS)   # send the query to the Guardian API\n",
    "response_dict = response.json()['response']            # keep the relevant component of the response\n",
    "\n",
    "# Fill the list of words counts for the second search\n",
    "for resp in _____['results']:\n",
    "    wordcount = _____['fields']['wordcount']\n",
    "    trump_counts.append(int(wordcount))\n",
    "\n",
    "# Calculate the average of the word counts\n",
    "morrison_avg = np.mean(morrison_counts)        \n",
    "print(morrison_avg)\n",
    "trump_avg = np.mean(trump_counts)\n",
    "print(trump_avg)\n",
    "\n",
    "# Store the result in the assignment variable\n",
    "_04 = [morrison_avg, trump_avg] # do not change this variable name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"#49699E\">PART B | CHAPTER 7 - Web Scraping</font>\n",
    "<a id=\"SECBEP\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "As you work through the questions in Part B, we encourage you to open the 2019 Federal Elections Results webpage (link provided in Question 9) in your browser, with developer tools enabled.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5:\n",
    "<div class=\"alert alert-block alert-info\">  \n",
    "For this exercise, we're going to use the power of scraping to delve into the results of the 2019 Canadian Federal Election! Just as every journey begins with a single step, we're going to start out with some basics. \n",
    "</div>\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Use the URL for Wikipedia's riding-by-riding election results to retrieve the web page and then check to see the if the result from the server was ok.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_05\n",
    "\n",
    "# Store our website address in the 'url' variable\n",
    "url = \"https://en.wikipedia.org/wiki/Results_of_the_2019_Canadian_federal_election\"\n",
    "\n",
    "# Retrieve the website\n",
    "r = requests._____(url)\n",
    "\n",
    "# Query as to whether or not our request was 'ok', and store the result\n",
    "request_okay = r._____ \n",
    "\n",
    "# Report result to the assignment variable\n",
    "_05 = request_okay # do not change this variable name\n",
    "\n",
    "# Display result\n",
    "print(request_okay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">  \n",
    "Now that we have the HTML in hand, let's use BeautifulSoup to process the website and get its on-screen title (the text that's immediately above the 'From Wikipedia, the free encyclopedia'). Since the on-screen title differs somewhat from the tab title that we retreived in the chapter on scraping, you might need to do a little digging in the site's HTML to figure out where it's stored. (Hint: you can find it in the 'body' of the article, and it is a type of heading)\n",
    "</div>\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Process the website using BeautifulSoup and find the on-screen title of the website we retrieved. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_06\n",
    "\n",
    "# Use BeautifulSoup to create an HTML DOM\n",
    "soup = BeautifulSoup(r.content, 'lxml')\n",
    "\n",
    "# Use the soup object to find the text of the web page's on-screen title\n",
    "on_screen_title = soup._____(\"h1\")[0].text\n",
    "\n",
    "# Store the result in the assignment variable\n",
    "_06 = on_screen_title # do not change this variable name\n",
    "\n",
    "# Display Result\n",
    "print(on_screen_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">  \n",
    "If you scroll around on the webpage we're scraping information from, you might notice that the centrepiece of the page is a large table entitled 'Results by riding - 2019 Canadian federal election'. It contains a whole lot of data; it would be great to have access to all of it in an organized fashion! Fortunately, we can easily find tables in this article by searching for objects with the 'table' tag. Unfortunately, there are 27 such tables in the article, and we can't be certain of the order they appear in BeautifulSoup's 'findAll' results. There are many ways to programmatically ensure that you've got the correct table. In this question, we're going to take advantage of the fact that we know the name of the table we're looking for. \n",
    "</div>\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Iterate through the HTML tables in the web page to locate the \"Results by riding\" table. Store its position in the list as an integer.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_07\n",
    "\n",
    "# Get a list of all the tables in the web page\n",
    "list_of_tables = soup.findAll(\"table\")\n",
    "\n",
    "# Initialize the variable we'll use to store the index\n",
    "result_table_index = None\n",
    "\n",
    "# Iterate over each table in the wikipedia article\n",
    "for i, table in enumerate(list_of_tables):\n",
    "    first_row = table._____('tr')[0].text # Get the first row of the table\n",
    "    if \"Results by riding - 2019 Canadian federal election\" in _____:\n",
    "        result_table_index = i  # If we get a match, we've found our table!\n",
    "        \n",
    "# Store the index of the result table in the assignment variable\n",
    "_07 = result_table_index # do not change this variable name\n",
    "\n",
    "# Display index of the result table\n",
    "print(result_table_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">  \n",
    "Now that we have the index of the table we want, we can easily retrieve it from our `list_of_tables`. All of the juicy, riveting details of the election are almost within reach, but before we can grasp them, we're going to need to know a bit more about how the data is organized. Since this is an HTML table, you can think of it as being built from a large number of 'rows' in the table. Take some time to play around with the table in your browser using development tools. We're interested in figuring out what HTML tag is use to denote a single row in the table (take, for example, Calgary Nose Hill - what's the tag that is used to identify its entire row in the table? Make sure you're inside the table's 'tbody' tag, and not its 'thead' tag).  \n",
    "</div>\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Find the HTML tag used to designate a single row of data in this table.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#_08\n",
    "\n",
    "# Store the two-letter HTML tag you found as part of your investigation (in string format)\n",
    "row_tag = _____\n",
    "\n",
    "# Store the result in the assignment variable\n",
    "_08 = row_tag # do not change this variable name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">  \n",
    "Now that we have access to the rows of the table, we can look through them to find all of the data corresponding to any given riding! We might as well keep things close to 'home'; in the following code cell, we're going to produce a list of all the rows in the table's body and then locate the row corresponding to the 'Waterloo' riding. \n",
    "</div>\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Create a list of all of the rows in the 'Results by riding' table, and then locate the 'Waterloo' riding's row. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_09\n",
    "\n",
    "# Using the index we found in the previous question, retrieve the \"Results by riding\" table from 'list_of_tables'\n",
    "results_table = list_of_tables[_____]\n",
    "\n",
    "# Create a list of all rows in the table by searching for the row tag you found\n",
    "all_rows = results_table.findAll(row_tag)\n",
    "\n",
    "# Iterate through list of rows to find the 'Waterloo' string in their text.\n",
    "for row in all_rows:\n",
    "    if \"Waterloo\" in row._____:\n",
    "        waterloo_row = row\n",
    "        \n",
    "# Process the results into a human-readable form:\n",
    "waterloo_text = [r for r in waterloo_row.text.split(\"\\n\") if r]\n",
    "\n",
    "# Store the result in the assignment variable\n",
    "_09 = waterloo_text # do not change this variable name\n",
    "\n",
    "print(waterloo_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 10:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">  \n",
    "If everything went well, we should now have access to the 'waterloo_text' variable, which is a list of 18 strings we created from the Riding of Waterloo's row in the 'Results by riding' table. Fortunately for us, all of the other data rows should follow the exact same pattern. We can use this inter-row regularity to create a Pandas dataframe that should closely match the table in the wikipedia article. \n",
    "</div>\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Remove the first 5 rows of the 'all_rows' variable (index positions 0 to 4). Then, populate the pandas dataframe (we've filled in the column names already) with the rows from the table you scraped. Finally, find the number of ridings from each province and territory.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_10:\n",
    "\n",
    "\n",
    "# Remove first 5 items from the 'all_rows' list\n",
    "all_but_5_rows = all_rows[_____:None]\n",
    "\n",
    "# Initialize a list of the columns from the Wikipedia table\n",
    "riding_columns = [\n",
    "    'riding',\n",
    "    'province_or_territory',\n",
    "    '2015_winning_party',\n",
    "    '2019_winning_party',\n",
    "    'votes',\n",
    "    'share',\n",
    "    'margin_num',\n",
    "    'margin_pct',\n",
    "    'turnout',\n",
    "    'liberal',\n",
    "    'conservative',\n",
    "    'ndp',\n",
    "    'bloc',\n",
    "    'green',\n",
    "    'ppc',\n",
    "    'independent',\n",
    "    'other',\n",
    "    'total',\n",
    "    'riding_url',\n",
    "]\n",
    "\n",
    "# Initialize dataframe\n",
    "riding_df = pd.DataFrame(columns=riding_columns)\n",
    "\n",
    "# Populate dataframe with rows\n",
    "for row in all_but_5_rows:\n",
    "    row_text = [r for r in row.text.replace(',','').split(\"\\n\") if r] # Get the text of each row\n",
    "    row_text.append(row.find('a', href=True)['href']) # Explicitly add the link to the riding page contained in the first column\n",
    "\n",
    "    while len(row_text) < 19:\n",
    "        row_text.insert(-2, 0) # This fixes 3 broken rows\n",
    "\n",
    "    df_row = pd.Series(row_text, index=riding_df.columns)\n",
    "    riding_df = riding_df.append(df_row, ignore_index=True)\n",
    "    \n",
    "\n",
    "# Count the number of ridings from each province and territory\n",
    "riding_counts = riding_df['province_or_territory']._____()\n",
    "\n",
    "\n",
    "# Store the result in the assignment variable\n",
    "_10 = riding_counts # do not change this variable name\n",
    "\n",
    "riding_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 11:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">  \n",
    "If you're not a student of Canadian politics, it might surprise you to learn that in the 2019 election, the incumbent Liberal Party of Canada (Lib) recieved fewer votes than the Conservative Party of Canada (Con), and yet the Liberals won more seats than the Conservatives and formed government. This happened because each riding in the Canadian electoral system runs according to a winner-takes-all logic, where 100% of the rewards go to the cadidate who finished in first place, even if they only beat the person in second place by a single vote. This means that having a large margin of victory in a single riding is not desirable - presumably, that represents time, resources, and effort that could have been more efficiently allocated. In this question, we're going to see if we can use the data we scraped to help shed some light on why this strange state of affairs came to pass.\n",
    "</div>\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Examine the twenty ridings with the highest margin of victory. Then, examine the twenty ridings with the lowest margin of victory. The margin of victory is contained in the 'margin_num' and 'margin_pct' columns. Write a 1 or 2-sentence analysis of what you notice about the parties represented in the '2019_winning_party' column, and what it might say about how the Liberals won the election despite winning fewer votes overall.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_11\n",
    "\n",
    "# Convert the margin_num column to a numeric column\n",
    "riding_df['margin_num'] = pd.to_numeric(riding_df['margin_num'])\n",
    "\n",
    "# Sort the entire dataframe by the value of the margin number, ascending\n",
    "margin_df_ascending  = riding_df.sort_values(['margin_num'], _____=True)\n",
    "\n",
    "# display the 20 ridings with the largest margin of victory\n",
    "display(HTML('<div class=\"alert alert-block alert-info\">Ridings with highest margin of victory</div>'))\n",
    "display(margin_df_ascending.tail(20))\n",
    "\n",
    "# display the 20 ridings with the lowest margin of victory\n",
    "display(HTML('<div class=\"alert alert-block alert-danger\">Ridings with lowest margin of victory</div>'))\n",
    "display(margin_df_ascending.head(20))\n",
    "\n",
    "# Write your answer in the space between the two pairs of triple quote marks: \n",
    "_11 = \"\"\"\n",
    "\n",
    "Sentence one of your analysis goes here.\n",
    "Sentence two of your analysis goes here. \n",
    "\n",
    "\"\"\" # do not change this variable name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 12:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">  \n",
    "For this final problem, we're going to harness the information we've already gathered to create a scraper that's capable of semi-autonomously traversing a (vanishingly small) proportion of Wikipedia's pages. Specifically, we're going to take advantage of the fact that each of the riding entries that we scraped contains a link to Wikipedia's article on that riding. \n",
    "</div>\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Create a scraper capable of retrieving the 'Date created' date for each of the 20 ridings with the lowest margin of victory. Average all of these dates and round them to the nearest full year. Make certain that your scraper is capable of handling riding articles that do not contain a 'First Contested' date.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#_12\n",
    "\n",
    "# Initialize variables\n",
    "link_base = \"https://en.wikipedia.org/\"\n",
    "year_created_list = []\n",
    "\n",
    "# retrieve 'District Created' for each link in list\n",
    "for i, row in list(margin_df_ascending.iterrows())[0:20]:\n",
    "    sleep(0.5)\n",
    "    r = _____._____(link_base + row['riding_url']) # Send request to Wikipedia\n",
    "    soup = _____(r._____, 'lxml') # Process using BeautifulSoup\n",
    "    for row in soup.findAll('tr'): # Iterate over our 'soup' DOM\n",
    "        if 'District created' in row.text: # If we find a match, add the value.\n",
    "            year_created_list.append(int(row.find('td').text))\n",
    "            \n",
    "# Find the average year of creation and round it to the nearest full year\n",
    "avg_creation_year =round(sum(year_created_list)/len(year_created_list))\n",
    "\n",
    "# Store the result in the assignment variable\n",
    "_12 = avg_creation_year # do not change this variable name\n",
    "\n",
    "print(avg_creation_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "## FINALIZE ASSIGNMENT\n",
    "\n",
    "import pickle as pkl\n",
    "\n",
    "module = 4\n",
    "\n",
    "response_dict = {\n",
    "    \"student_id\": student_id,\n",
    "    \"grad_student\": \"UE_undergrad\",\n",
    "    \"module\": module,\n",
    "    \"responses\": [\n",
    "        _01,\n",
    "        _02,\n",
    "        _03,\n",
    "        _04,\n",
    "        _05,\n",
    "        _06,\n",
    "        _07,\n",
    "        _08,\n",
    "        _09,\n",
    "        _10,\n",
    "        _11,\n",
    "        _12,\n",
    "    ],\n",
    "    \"code_cells\": In\n",
    "}\n",
    "\n",
    "module_string = f\"module_{response_dict['module']}\"\n",
    "filename = f\"{module_string}__student_{student_id}.pkl\"\n",
    "\n",
    "with open(f\"./{filename}\", 'wb') as stream:\n",
    "    pkl.dump(response_dict, stream)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
